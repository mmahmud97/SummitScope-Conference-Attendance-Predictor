{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA PREPROCESSING\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "''' Feature Selection with drop'''\n",
    "def drop(df):\n",
    "    drop_cols = [\n",
    "        \"email\",\n",
    "        'Current_membership_service_ID',\n",
    "        'Cust_Subsidiary',\n",
    "        'Subsidiary_ID',\n",
    "        'Subsidiary',\n",
    "        'billaddr_city',\n",
    "        'billaddr_zip',\n",
    "        'billaddr_country',\n",
    "        'billaddr_country_desc',\n",
    "        'Customer_ExtID',\n",
    "        'probability_lapsed',\n",
    "    ]\n",
    "    print(f\"dropped columns: {len(drop_cols)} out of {len(df.columns)}\")\n",
    "    df.drop(drop_cols, axis=1, inplace=True, errors='ignore')  # added errors='ignore' to handle if columns are not present\n",
    "    return df\n",
    "\n",
    "def read_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a CSV file and returns a DataFrame.\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "def handle_missing_values(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Handles missing values in the DataFrame.\n",
    "    \"\"\"\n",
    "    df['distance'] = df['distance'].replace(-1000, np.NaN)  # Reverse \"-1000\" in \"distance\" to nan\n",
    "    return df\n",
    "\n",
    "def categorical_imputer(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes missing values in categorical columns.\n",
    "    \"\"\"\n",
    "    New_cate_Fill = [\"Stadard_Job_Title\", \"gender\", \"ethnicity\", \"education\",\n",
    "                     \"organization_unit\", \"organization_type\",\n",
    "                     \"native_language\", \"organization_unit\", \"organization_type\",\n",
    "                     \"Industry_Category\",\n",
    "                     \"Emp_Exemption_Status\",\n",
    "                     \"native_language\"]\n",
    "\n",
    "    Major_Fill = [\"department_size\", \"company_size\", \"employee_oversee\", \"Monthly_Donor\"]  # MODE\n",
    "\n",
    "    # Remove the line that tries to access 'Customer_ID' after it has been removed\n",
    "    # customer_ids_test = df['Customer_ID'].copy()\n",
    "\n",
    "    to_remove = [\"billaddr_state\",\n",
    "                 \"Customer_ID\",\n",
    "                 \"native_language\"]  # drop ID\n",
    "\n",
    "    for c in New_cate_Fill:\n",
    "        df[c] = df[c].fillna('new_cate')\n",
    "\n",
    "    for c in Major_Fill:\n",
    "        major_category = df[c].value_counts().keys()[0]\n",
    "        df[c] = df[c].fillna(major_category)\n",
    "\n",
    "    for c in to_remove:\n",
    "        if c in df.columns:\n",
    "            df.drop(c, axis=1, inplace=True)\n",
    "\n",
    "    if \"employee_oversee\" in df.columns:\n",
    "        df.drop(\"employee_oversee\", axis=1, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def datetime_imputer(df: pd.DataFrame, date_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Imputes missing values in datetime columns and converts them to a numerical format.\n",
    "    \"\"\"\n",
    "    for c in date_cols:\n",
    "        df[c] = pd.to_datetime(df[c])\n",
    "        df[c] = (datetime.datetime.now() - df[c]).apply(lambda x: x.days)\n",
    "\n",
    "    impute_membership_paid_through = df.membership_paid_through.max() + 365\n",
    "    impute_SHRM_Join_Date = -365\n",
    "    impute_Current_Start_Date = df.Current_Start_Date.min() - 365\n",
    "    impute_likely_next_transaction = df.likely_next_transaction.min() - 365\n",
    "\n",
    "    df['membership_paid_through'] = df['membership_paid_through'].fillna(impute_membership_paid_through)\n",
    "    df['SHRM_Join_Date'] = df['SHRM_Join_Date'].fillna(impute_SHRM_Join_Date)\n",
    "    df['Current_Start_Date'] = df['Current_Start_Date'].fillna(impute_Current_Start_Date)\n",
    "    df['likely_next_transaction'] = df['likely_next_transaction'].fillna(impute_likely_next_transaction)\n",
    "\n",
    "    return df\n",
    "\n",
    "def define_cols(df: pd.DataFrame) -> tuple:\n",
    "    \"\"\"\n",
    "    Defines the type of the variables.\n",
    "    \"\"\"\n",
    "    cate_cols = ['Stadard_Job_Title', 'DISALLOW_ALL_COMMUNICATION',\n",
    "                 'disallow_email_communication', 'disallow_phone_communication',\n",
    "                 'disallow_regular_mail_communi', 'disallow_third_party_communic',\n",
    "                 'department_size', 'gender', 'ethnicity', 'education',\n",
    "                 'company_size', 'native_language', 'organization_unit',\n",
    "                 'organization_type', 'employee_oversee', 'shrm_organization_multination',\n",
    "                 'shrm_organization_unionized', 'billaddr_state', 'Is_Person',\n",
    "                 'IsInActive', 'LoginAccess', 'Monthly_Donor',\n",
    "                 'Customer_Segment', 'if_member', 'if_active_member',\n",
    "                 'Emp_Exemption_Status', 'Industry_Category']\n",
    "\n",
    "    date_cols = ['membership_paid_through', 'SHRM_Join_Date', 'Current_Start_Date',\n",
    "                 'first_order_date', 'last_order_date', 'likely_next_transaction']\n",
    "\n",
    "    num_cols = ['retina_clv_10yr', 'retina_clv_1yr', 'retina_clv_2yr', 'retina_clv_3yr',\n",
    "                'retina_clv_5yr', 'retina_clv_3mo', 'retina_clv_6mo', 'retina_clv_9mo',\n",
    "                'retina_residual_value_10yr', 'retina_residual_value_1yr',\n",
    "                'retina_residual_value_2yr', 'retina_residual_value_3yr',\n",
    "                'retina_residual_value_5yr', 'retina_residual_value_3mo',\n",
    "                'retina_residual_value_6mo', 'retina_residual_value_9mo',\n",
    "                'total_num_orders', 'total_order_value', 'typical_order_value',\n",
    "                'probability_alive', #'distance', ###undo for annual conference\n",
    "                'SHRM_ENTERED_PROFESSION']\n",
    "\n",
    "    label = ['attend']\n",
    "\n",
    "    ID = ['Customer_ID']\n",
    "\n",
    "    return cate_cols, date_cols, num_cols, label, ID\n",
    "\n",
    "#This function is for feature selection dimensionality reduction. It removes some features manually before perform feature selection. Works best!\n",
    "def data_clean(df: pd.DataFrame) -> tuple:\n",
    "\n",
    "    df = drop(df)  # Drop specified columns\n",
    "\n",
    "    # Extract and store Customer_ID before preprocessing\n",
    "    customer_ids = df['Customer_ID'].copy()\n",
    "\n",
    "    # Drop the 'Customer_ID' column from the DataFrame for further processing\n",
    "    df.drop('Customer_ID', axis=1, inplace=True)\n",
    "\n",
    "    cate_cols, date_cols, num_cols, label, ID = define_cols(df)\n",
    "    df = categorical_imputer(df)\n",
    "    df = datetime_imputer(df, date_cols)\n",
    "    pd.set_option('future.no_silent_downcasting', True)\n",
    "    df['attend'] = df['attend'].replace({\"Yes\": 1, \"No\": 0})\n",
    "\n",
    "    cols = df.columns.tolist()\n",
    "    cols.remove(\"attend\")\n",
    "\n",
    "    num_index = []\n",
    "    cat_index = []\n",
    "    numeric_features = []\n",
    "    categorical_features = []\n",
    "    for i, col in enumerate(cols):\n",
    "        if col in cate_cols:\n",
    "            cat_index.append(i)\n",
    "            categorical_features.append(col)\n",
    "        else:\n",
    "            num_index.append(i)\n",
    "            numeric_features.append(col)\n",
    "\n",
    "    X = df[cols]\n",
    "\n",
    "    return X, df['attend'], numeric_features, categorical_features, customer_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL EVALUATION\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, f1_score, precision_score\n",
    "\n",
    "def evaluate_basic_metrics(y_true, y_pred, propensity_scores):\n",
    "    \"\"\"\n",
    "    Evaluate basic metrics including AUC, Recall, and Confusion Matrix.\n",
    "    \"\"\"\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    print(f\"*****  AUC = {auc:.2%}    Recall ={recall:.2%}  ***** \\n\")\n",
    "    print('\\nConfusion Matrix:\\n', confusion)\n",
    "\n",
    "    return auc, recall, confusion\n",
    "\n",
    "\n",
    "def plot_roc_curve(y_true, propensity_scores):\n",
    "    \"\"\"\n",
    "    Plot ROC curve for the model.\n",
    "    \"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, propensity_scores)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc_score(y_true, propensity_scores))\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(y_true, propensity_scores):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curve for the model.\n",
    "    \"\"\"\n",
    "    precision, recall, _ = precision_recall_curve(y_true, propensity_scores)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, label='Precision-Recall curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluate_model(clf, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensive model evaluation.\n",
    "    \"\"\"\n",
    "    # Basic metrics\n",
    "    propensity_scores = clf.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (propensity_scores > 0.5).astype('float')\n",
    "    auc, recall, confusion = evaluate_basic_metrics(y_test, y_pred, propensity_scores)\n",
    "\n",
    "    # ROC Curve\n",
    "    plot_roc_curve(y_test, propensity_scores)\n",
    "\n",
    "    # Precision-Recall Curve\n",
    "    plot_precision_recall_curve(y_test, propensity_scores)\n",
    "\n",
    "    # Additional metrics can be added here as needed\n",
    "    return auc, recall, confusion\n",
    "\n",
    "# Additional evaluation functions and visualizations can be added here based on further requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL TRAINING\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, recall_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "def full_imputer(X, numeric_features, categorical_features):\n",
    "    \"\"\"Pipeline for preprocessing data: imputation and encoding.\"\"\"\n",
    "\n",
    "    # Convert numeric columns to float type and coerce errors (convert non-convertible values to NaN)\n",
    "    for col in numeric_features:\n",
    "        X.loc[:, col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value=-1)),\n",
    "        ('onehot', OneHotEncoder(drop='first'))\n",
    "    ])\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "    encoded = clf.fit_transform(X)\n",
    "\n",
    "    OneHot_features = clf.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(\n",
    "        categorical_features)\n",
    "    features = numeric_features + OneHot_features.tolist()\n",
    "\n",
    "    return encoded, features\n",
    "\n",
    "\n",
    "def train(X_encoded, y, features= None, model=\"Gradient Boosting\", test=True, return_importances = False):\n",
    "    \"\"\"Train the model based on the specified algorithm.\"\"\"\n",
    "\n",
    "    y = y.astype(int)\n",
    "\n",
    "    if model == 'Decision Tree':\n",
    "        clf = DecisionTreeClassifier(max_depth=None, random_state=42)\n",
    "        clf.fit(X_encoded, y)\n",
    "\n",
    "    elif model == \"Logistic Regression\":\n",
    "        clf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        clf.fit(X_encoded, y)\n",
    "\n",
    "    elif model == 'Gradient Boosting':\n",
    "        clf = LGBMClassifier(learning_rate=0.09, max_depth=-1, random_state=42, n_estimators=1000, num_leaves=40)\n",
    "\n",
    "        if test:\n",
    "            clf.fit(X_encoded, y)\n",
    "        else:\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n",
    "            clf.fit(X_train, y_train, eval_set=[(X_test, y_test), (X_train, y_train)], eval_metric='auc')\n",
    "\n",
    "    if return_importances:\n",
    "        return clf, get_feature_importances(clf, features)\n",
    "\n",
    "    return clf\n",
    "\n",
    "def evaluate_model(clf, X_test, y_test):\n",
    "    \"\"\"Evaluate the model and print metrics.\"\"\"\n",
    "\n",
    "    # Predict probabilities for the positive class\n",
    "    propensity_scores = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Generate binary predictions based on a default threshold of 0.5\n",
    "    predictions = (propensity_scores > 0.5).astype(int)\n",
    "\n",
    "    # Calculate AUC using the probability scores\n",
    "    y_test = y_test.astype(int)\n",
    "    auc_score = roc_auc_score(y_test, propensity_scores)\n",
    "\n",
    "    # Calculate recall using the binary predictions\n",
    "    recall = recall_score(y_test, predictions)\n",
    "\n",
    "    # Print the basic evaluation metrics\n",
    "    print(f\"***** AUC = {auc_score:.2%}    Recall = {recall:.2%}  ***** \\n\")\n",
    "\n",
    "    # Threshold analysis\n",
    "    thresholds = [0.5, 0.4, 0.3, 0.2, 0.1, 0.01, 0.005, 0.001, 0.0001]\n",
    "    auc_list, recall_list, target_list, confusion_matrices = [], [], [], []\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        # Generate binary predictions based on the current threshold\n",
    "        y_pred_threshold = (propensity_scores > threshold).astype(int)\n",
    "\n",
    "        # Calculate metrics for the current threshold\n",
    "        auc_list.append(roc_auc_score(y_test, y_pred_threshold))\n",
    "        recall_list.append(recall_score(y_test, y_pred_threshold))\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_threshold).ravel()\n",
    "        target_list.append((fp + tp) / len(y_test))\n",
    "        confusion_matrices.append(f\"[{tn},{fp}],[{fn},{tp}]\")\n",
    "\n",
    "    # Prepare and print the summary table\n",
    "    summary_table = pd.DataFrame({\n",
    "        'Threshold': thresholds,\n",
    "        'AUC:': [f\"{x:.2%}\" for x in auc_list],\n",
    "        'Recall:': [f\"{x:.2%}\" for x in recall_list],\n",
    "        'Target %': [f\"{x:.3%}\" for x in target_list],\n",
    "        'Confusion Matrix': confusion_matrices\n",
    "    })\n",
    "    print(summary_table)\n",
    "\n",
    "\n",
    "# Additional functions and logic can be added here based on further requirements.\n",
    "def get_feature_importances(clf, features):\n",
    "    \"\"\"Get feature importances from the trained model.\"\"\"\n",
    "    importances = clf.feature_importances_\n",
    "    sorted_idx = np.argsort(importances)[::-1]\n",
    "    return np.array(features)[sorted_idx], importances[sorted_idx]\n",
    "\n",
    "\n",
    "def apply_lda(X_train, y_train, n_components=None):  # <-- New function\n",
    "    lda = LinearDiscriminantAnalysis(n_components=n_components)\n",
    "    X_train_lda = lda.fit_transform(X_train.toarray(), y_train)\n",
    "    return X_train_lda, lda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN MODEL NO TEST - NO PREDICTIONS\n",
    "\n",
    "'''This is for Feature Selection'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_preprocessing import read_csv, data_clean\n",
    "from model_training import full_imputer, train, evaluate_model\n",
    "from output_handling import save_propensity_scores\n",
    "\n",
    "def main():\n",
    "    df = read_csv('files/ANN24.csv')\n",
    "\n",
    "    # Preprocess the data\n",
    "    X, y, numeric_features, categorical_features, customer_ids = data_clean(df)\n",
    "    print(1)\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, customer_ids_train, customer_ids_test = train_test_split(X, y, customer_ids, test_size=0.2, random_state=42)\n",
    "    print(2)\n",
    "    # Preprocess the training data\n",
    "\n",
    "    X_train_encoded, features = full_imputer(X_train, numeric_features, categorical_features)\n",
    "\n",
    "    print(\"y_train type: \",y_train.dtype)\n",
    "    y_train = y_train.astype(int)\n",
    "\n",
    "    # Train the model and get feature importances\n",
    "    clf, (sorted_features, importances) = train(X_train_encoded, y_train, features=features, model=\"Decision Tree\", test=False, return_importances=True)\n",
    "\n",
    "    # Select top N features (for example, top 50 or you can set a threshold)\n",
    "    selected_features = sorted_features[:65]\n",
    "    print(selected_features)\n",
    "\n",
    "    # Update X_train_encoded and X_test_encoded to only include selected features\n",
    "    feature_indices = [features.index(feat) for feat in selected_features]\n",
    "    X_train_encoded = X_train_encoded[:, feature_indices]\n",
    "    X_test_encoded, _ = full_imputer(X_test, numeric_features, categorical_features)\n",
    "    X_test_encoded = X_test_encoded[:, feature_indices]\n",
    "\n",
    "    # Retrain the model using only the selected features\n",
    "    clf = train(X_train_encoded, y_train, model=\"Gradient Boosting\", test=False)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(clf, X_test_encoded, y_test)\n",
    "\n",
    "    # Save propensity scores\n",
    "    propensity_scores = clf.predict_proba(X_test_encoded)[:, 1]\n",
    "    save_propensity_scores(customer_ids_test, propensity_scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "#This is for the normal model without feature selection and manual dropping of fields\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_preprocessing import read_csv, data_clean\n",
    "from model_training import full_imputer, train, evaluate_model\n",
    "#from model_evaluation import evaluate_model\n",
    "from output_handling import save_propensity_scores\n",
    "\n",
    "def main():\n",
    "\n",
    "    df = read_csv('trainDataSet.csv')\n",
    "\n",
    "    # Preprocess the data\n",
    "    X, y, numeric_features, categorical_features, customer_ids = data_clean(df)\n",
    "\n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test, customer_ids_train, customer_ids_test = train_test_split(X, y, customer_ids, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Preprocess the training data\n",
    "    X_train_encoded, features = full_imputer(X_train, numeric_features, categorical_features)\n",
    "\n",
    "    # Train the model\n",
    "    clf = train(X_train_encoded, y_train, model=\"Gradient Boosting\", test=False)\n",
    "\n",
    "    # Preprocess the test data\n",
    "    X_test_encoded, _ = full_imputer(X_test, numeric_features, categorical_features)\n",
    "\n",
    "    # Evaluate the model\n",
    "    evaluate_model(clf, X_test_encoded, y_test)\n",
    "\n",
    "    # Save propensity scores\n",
    "    # propensity_scores = clf.predict_proba(X_test_encoded)[:, 1]\n",
    "    # save_propensity_scores(customer_ids_test, propensity_scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RUN MODEL YEST TEST - OUTPUT IS PREDICTIONS\n",
    "\n",
    "# To incorporate Feature Selection in this file we have the following code\n",
    "import pandas as pd\n",
    "from data_preprocessing import read_csv, data_clean\n",
    "from model_training import full_imputer, train, evaluate_model\n",
    "from output_handling import save_propensity_scores\n",
    "\n",
    "def main():\n",
    "    # Load training and testing data\n",
    "    train_file_path = \"files/ANN23.csv\"\n",
    "    test_file_path = \"files/TestANN24.csv\"\n",
    "\n",
    "\n",
    "    train_df = pd.read_csv(train_file_path,low_memory=False)\n",
    "    test_df = pd.read_csv(test_file_path,low_memory=False)\n",
    "    print(\"Files read\")\n",
    "\n",
    "    # Preprocess training data\n",
    "    X_train, y_train, numeric_features_train, categorical_features_train, _ = data_clean(train_df)\n",
    "    X_train_encoded, features_train = full_imputer(X_train, numeric_features_train, categorical_features_train)\n",
    "\n",
    "    print(\"preprocess complete\")\n",
    "\n",
    "    y_train = y_train.astype(int)\n",
    "    # Train the model on training data and get feature importances\n",
    "    clf, (sorted_features, importances) = train(X_train_encoded, y_train, features=features_train, model=\"Gradient Boosting\",return_importances=True)\n",
    "    print(\"training - training complete\")\n",
    "    # Select top N features (for example, top 50 or you can set a threshold)\n",
    "    selected_features = sorted_features[:65]  # <-- This number can be adjusted based on your needs\n",
    "    print(selected_features)\n",
    "\n",
    "    # Update X_train_encoded to only include selected features\n",
    "    feature_indices = [features_train.index(feat) for feat in selected_features]\n",
    "    X_train_encoded = X_train_encoded[:, feature_indices]\n",
    "\n",
    "    # Preprocess testing data\n",
    "    X_test, y_test, numeric_features_test, categorical_features_test, customer_ids = data_clean(test_df)\n",
    "    X_test_encoded, features_test = full_imputer(X_test, numeric_features_test, categorical_features_test)\n",
    "    X_test_encoded = X_test_encoded[:, feature_indices]  # Use the same selected features for the test set\n",
    "\n",
    "    # Retrain the model on training data using only the selected features\n",
    "    clf = train(X_train_encoded, y_train)\n",
    "\n",
    "    y_test = y_test.astype(int)\n",
    "    # Evaluate the model on testing data\n",
    "    evaluate_model(clf, X_test_encoded, y_test)\n",
    "\n",
    "    # Save and visualize propensity scores\n",
    "    propensity_scores = clf.predict_proba(X_test_encoded)[:, 1]\n",
    "    save_propensity_scores(customer_ids, propensity_scores)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SHARING WITH MARKETING TEAM - FILE PREPARATION\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Read your main dataframe\n",
    "df = pd.read_csv('propensity_scores.csv')\n",
    "\n",
    "# Define bins and labels\n",
    "bins = [0.004, 0.01, 0.1, 0.2, 0.4, 1]\n",
    "labels = [\"5_Next_Best\", \"4_Very_Low\", \"3_Low\", \"2_Medium\", \"1_High\"]\n",
    "\n",
    "# Create a new 'Propensity_Level' column in df\n",
    "df['Propensity_Level'] = pd.cut(df['Propensity'], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Filter out rows with NaN in 'Propensity_Level' column\n",
    "df = df.dropna(subset=['Propensity_Level'])\n",
    "\n",
    "# Order the DataFrame by 'Propensity_Level'\n",
    "df = df.sort_values(by='Propensity_Level', ascending=False)\n",
    "\n",
    "# Read XFILE\n",
    "xfile_df = pd.read_csv('files/ANN24.csv')\n",
    "\n",
    "# Merge df with xfile_df on 'Customer_ID'\n",
    "merged_df = pd.merge(df, xfile_df, on='Customer_ID', how='inner')\n",
    "\n",
    "# Get the current UTC time in the ISO 8601 format as required by Marketo\n",
    "#current_date_marketo_format = datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "current_date_marketo_format = datetime.utcnow().strftime('%Y-%m-%d')\n",
    "\n",
    "# Convert the string to a datetime object and assign it to your DataFrame\n",
    "merged_df['Propensity_Date'] = pd.to_datetime(current_date_marketo_format)\n",
    "\n",
    "# Drop 'Customer_ID' column and keep 'email' and 'Propensity_Level' columns\n",
    "final_df = merged_df[['email', 'Propensity_Level', 'Propensity_Date']]\n",
    "\n",
    "# Save to CSV\n",
    "folder_path = \"S:/Collaboration/Business Intelligence/Muzammil-E-Haque Mahmud/feb23-June23/CAR/PropensityListsSendsDirectory/\"\n",
    "\n",
    "current_date_formatted = datetime.utcnow().strftime('%Y_%m_%d')\n",
    "\n",
    "# Create the file name by concatenating the string \"PropensityList_\" with the formatted date\n",
    "file_name = f\"{folder_path}PropensityList_{current_date_formatted}.csv\"\n",
    "\n",
    "# Export your DataFrame to CSV using the constructed file name\n",
    "final_df.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
